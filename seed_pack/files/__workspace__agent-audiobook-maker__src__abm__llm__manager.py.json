{
  "file": "/workspace/agent-audiobook-maker/src/abm/llm/manager.py",
  "module": "abm.llm",
  "summary": "Utilities to manage a local or remote LLM service. This module provides a small wrapper around an OpenAI-compatible endpoint\nsuch as `ollama`.",
  "top_level_symbols": [
    {
      "name": "LLMBackend",
      "kind": "class",
      "methods": [
        "headers"
      ],
      "doc": "Configuration for a local or remote LLM endpoint. Attributes:\n    kind: Backend type, e.g."
    },
    {
      "name": "LLMService",
      "kind": "class",
      "methods": [
        "__init__",
        "is_alive",
        "ensure_up",
        "_wait_ready",
        "stop",
        "pull_model"
      ],
      "doc": "Manage the lifecycle of a local LLM server. Attributes:\n    backend: Connection information for the target service."
    }
  ],
  "io_contracts": {
    "inputs": [],
    "outputs": [],
    "file_patterns_written": [],
    "stdout_side_effects": []
  },
  "dependencies": {
    "imports_internal": [],
    "imports_external": [
      "__future__",
      "dataclasses",
      "logging",
      "os",
      "requests",
      "signal",
      "subprocess",
      "time"
    ]
  },
  "errors_raised": [
    {
      "type": "TimeoutError",
      "when": "Unknown"
    }
  ],
  "env_vars": []
}